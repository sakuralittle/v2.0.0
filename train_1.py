import backtrader as bt
import pandas as pd
import numpy as np
import os
import time
from datetime import datetime
from indicator import IndicatorFactory
from agent import create_sac_agent, ReplayBuffer
from config import SAC_CONFIG, DATA_CONFIG, BROKER_CONFIG, TRAIN_CONFIG
from collections import deque

class TrainingStrategy(bt.Strategy):
    """è¨“ç·´ç”¨ç­–ç•¥ - èˆ‡ train_agent_indicator äº’å‹•"""
    params = (
        ('trainer', None),  # train_agent_indicator å¯¦ä¾‹
        ('agent', None),     # SAC agent å¯¦ä¾‹
        ('train_mode', True),
    )
    
    def __init__(self):
        # åˆå§‹åŒ–æŠ€è¡“æŒ‡æ¨™ï¼ˆèˆ‡ agent_indicator ç›¸åŒï¼‰
        self.hullma = IndicatorFactory.hullma_indicator(self.data)
        self.rsi = IndicatorFactory.rsi(self.data, period=SAC_CONFIG['rsi_period'])
        self.cci = IndicatorFactory.cci(self.data, period=SAC_CONFIG['cci_period'])
        
        # ç‹€æ…‹æ­£è¦åŒ–
        self.normalization_window = SAC_CONFIG.get('normalization_window', 100)
        self.state_history = deque(maxlen=self.normalization_window)
        
        # ç¶“é©—æ”¶é›†
        self.current_state = None
        self.current_action = None
        self.step_count = 0
        self.episode_reward = 0
    
    def _build_state(self):
        """æ§‹å»ºç‹€æ…‹å‘é‡ï¼ˆèˆ‡ agent_indicator ç›¸åŒï¼‰"""
        current_price = self.data.close[0]
        mhull = self.hullma.mhull[0]
        shull = self.hullma.shull[0]
        signal = self.hullma.signal[0]
        rsi = self.rsi.rsi[0]
        cci = self.cci.cci[0]
        
        period = SAC_CONFIG['price_change_period']
        price_changes = []
        for i in range(1, period + 1):
            if len(self) > i:
                price_change = (self.data.close[-i+1] - self.data.close[-i]) / self.data.close[-i]
            else:
                price_change = 0
            price_changes.append(price_change)
        
        volume = self.data.volume[0]
        state_list = [mhull, shull, signal, volume, current_price]
        state_list.extend(price_changes)
        state_list.extend([rsi, cci])
        
        return np.array(state_list, dtype=np.float32)
    
    def _normalize_state(self, state):
        """Z-score æ­£è¦åŒ–"""
        self.state_history.append(state.copy())
        if len(self.state_history) < 2:
            return np.zeros_like(state)
        
        history_array = np.array(self.state_history)
        mean = np.mean(history_array, axis=0)
        std = np.std(history_array, axis=0)
        std = np.where(std < 1e-8, 1.0, std)
        normalized_state = (state - mean) / std
        return normalized_state
    
    def next(self):
        """æ¯æ ¹ K ç·šåŸ·è¡Œä¸€æ¬¡"""
        # æ§‹å»ºä¸¦æ­£è¦åŒ–ç‹€æ…‹
        raw_state = self._build_state()
        state = self._normalize_state(raw_state)
        
        # å¦‚æœæœ‰å‰ä¸€å€‹ç‹€æ…‹ï¼Œè¨ˆç®— reward ä¸¦å­˜å„²ç¶“é©—
        if self.current_state is not None and self.p.trainer is not None:
            reward = self.p.trainer.calculate_reward()
            self.episode_reward += reward
            
            # å­˜å„²ç¶“é©— (s, a, r, s', done=False)
            self.p.trainer.store_experience(
                self.current_state, 
                self.current_action, 
                reward, 
                state, 
                False
            )
            
            # è¨“ç·´ agent
            if self.step_count >= TRAIN_CONFIG['start_steps']:
                if self.step_count % TRAIN_CONFIG['update_frequency'] == 0:
                    self.p.trainer.train_agent(self.p.agent)
        
        # ä½¿ç”¨ agent é¸æ“‡å‹•ä½œ
        action = self.p.agent.select_action(state, evaluate=False)
        
        # åŸ·è¡Œäº¤æ˜“æ±ºç­–
        if not self.position:
            if action[0] > 0.5:  # è²·å…¥ä¿¡è™Ÿ
                self.buy()
        else:
            if action[0] < -0.5:  # è³£å‡ºä¿¡è™Ÿ
                self.close()
        
        # ä¿å­˜ç•¶å‰ç‹€æ…‹å’Œå‹•ä½œ
        self.current_state = state
        self.current_action = action
        self.step_count += 1
    
    def stop(self):
        """Episode çµæŸæ™‚èª¿ç”¨"""
        if self.p.trainer is not None:
            # è¨˜éŒ„æœ€çµ‚ reward
            final_reward = self.p.trainer.calculate_reward()
            self.episode_reward += final_reward
            
            # å­˜å„²æœ€å¾Œçš„ç¶“é©— (done=True)
            if self.current_state is not None:
                raw_state = self._build_state()
                next_state = self._normalize_state(raw_state)
                self.p.trainer.store_experience(
                    self.current_state,
                    self.current_action,
                    final_reward,
                    next_state,
                    True
                )
            
            # è¨˜éŒ„ episode çµ±è¨ˆ
            self.p.trainer.episode_rewards.append(self.episode_reward)
            portfolio_return = self.p.trainer.get_portfolio_return()
            self.p.trainer.episode_returns.append(portfolio_return)


class strategy1(bt.Strategy):
    """æ¨ç†ç”¨ç­–ç•¥ - ä½¿ç”¨é è¨“ç·´æ¨¡å‹"""
    def __init__(self):
        # åˆå§‹åŒ– SAC ä»£ç†æŒ‡æ¨™
        self.agent_ind = agent_indicator(self.data)
    
    def next(self):
        # å¾ agent_indicator ç²å–å‹•ä½œä¿¡è™Ÿ
        action = self.agent_ind.lines.agent_signal[0]
        
        # æ ¹æ“šä»£ç†çš„å‹•ä½œé€²è¡Œäº¤æ˜“æ±ºç­–
        if not self.position:
            # æ²’æœ‰æŒå€‰æ™‚
            if action > 0.5:  # è²·å…¥ä¿¡è™Ÿ
                self.buy()
        else:
            # å·²æœ‰æŒå€‰æ™‚
            if action < -0.5:  # è³£å‡ºä¿¡è™Ÿ
                self.close()

class train_agent_indicator():
    """
    SAC ä»£ç†æŒ‡æ¨™è¨“ç·´é¡åˆ¥
    
    åŠŸèƒ½ï¼š
    1. è¨“ç·´ SAC æ¨¡å‹
    2. è¨ˆç®— reward function
    3. ç®¡ç†è¨“ç·´éç¨‹
    """
    
    def __init__(self, cerebro, initial_cash=None):
        """
        åˆå§‹åŒ–è¨“ç·´ç’°å¢ƒ
        
        Args:
            cerebro: backtrader çš„ Cerebro å¯¦ä¾‹
            initial_cash: åˆå§‹è³‡é‡‘ï¼ˆè‹¥ç‚º Noneï¼Œå‰‡å¾ BROKER_CONFIG è®€å–ï¼‰
        """
        self.cerebro = cerebro
        self.initial_cash = initial_cash if initial_cash is not None else BROKER_CONFIG['initial_cash']
        self.previous_value = self.initial_cash
        
        # è¨“ç·´ç›¸é—œ
        self.replay_buffer = ReplayBuffer(TRAIN_CONFIG['replay_buffer_size'])
        self.episode_rewards = []
        self.episode_returns = []
        self.training_losses = []
        
    def get_current_portfolio_value(self):
        """
        ç²å–ç•¶å‰æŠ•è³‡çµ„åˆç¸½åƒ¹å€¼
        æ³¨æ„ï¼šæ­¤å€¼åŒ…å«ç¾é‡‘ + æŒå€‰å¸‚å€¼ï¼ˆå«æœªå¯¦ç¾æç›Šï¼‰
        
        Returns:
            float: ç•¶å‰ç¸½è³‡é‡‘ï¼ˆåŒ…å«æœªå¯¦ç¾æç›Šï¼‰
        """
        return self.cerebro.broker.getvalue()
    
    def get_portfolio_return(self):
        """
        ç²å–ç•¶å‰æŠ•è³‡çµ„åˆæ”¶ç›Šç‡
        
        Returns:
            float: æ”¶ç›Šç‡ï¼ˆç™¾åˆ†æ¯”ï¼Œä¾‹å¦‚ 0.05 ä»£è¡¨ 5%ï¼‰
        """
        current_value = self.get_current_portfolio_value()
        portfolio_return = (current_value - self.initial_cash) / self.initial_cash
        return portfolio_return
    
    def get_step_return(self):
        """
        ç²å–å–®æ­¥æ”¶ç›Šç‡ï¼ˆç›¸å°æ–¼ä¸Šä¸€æ­¥ï¼‰
        
        Returns:
            float: å–®æ­¥æ”¶ç›Šç‡
        """
        current_value = self.get_current_portfolio_value()
        step_return = (current_value - self.previous_value) / self.previous_value
        self.previous_value = current_value
        return step_return
    
    def calculate_reward(self, step_return=None):
        """
        è¨ˆç®—çå‹µå‡½æ•¸
        
        Args:
            step_return: å–®æ­¥æ”¶ç›Šç‡ï¼Œå¦‚æœç‚º None å‰‡è‡ªå‹•è¨ˆç®—
            
        Returns:
            float: çå‹µå€¼
        """
        if step_return is None:
            step_return = self.get_step_return()
        
        # ç°¡å–®çš„çå‹µå‡½æ•¸ï¼šç›´æ¥ä½¿ç”¨æ”¶ç›Šç‡
        # å¯ä»¥æ ¹æ“šéœ€æ±‚èª¿æ•´çå‹µå‡½æ•¸çš„è¨­è¨ˆ
        reward = step_return * 100  # æ”¾å¤§æ”¶ç›Šç‡ä»¥ä¾¿æ–¼è¨“ç·´
        
        return reward
    
    def store_experience(self, state, action, reward, next_state, done):
        """
        å­˜å„²ç¶“é©—åˆ° replay buffer
        
        Args:
            state: ç•¶å‰ç‹€æ…‹
            action: å‹•ä½œ
            reward: çå‹µ
            next_state: ä¸‹ä¸€ç‹€æ…‹
            done: æ˜¯å¦çµæŸ
        """
        self.replay_buffer.push(state, action, reward, next_state, done)
    
    def train_agent(self, agent, batch_size=None):
        """
        è¨“ç·´ agent
        
        Args:
            agent: SAC agent å¯¦ä¾‹
            batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆè‹¥ç‚º Noneï¼Œå‰‡å¾ TRAIN_CONFIG è®€å–ï¼‰
            
        Returns:
            dict: è¨“ç·´æå¤±
        """
        if batch_size is None:
            batch_size = TRAIN_CONFIG['batch_size']
        
        if len(self.replay_buffer) < batch_size:
            return None
        
        losses = agent.update(self.replay_buffer, batch_size)
        self.training_losses.append(losses)
        return losses
    
    def reset_episode_tracking(self):
        """
        é‡ç½® episode è¿½è¹¤è®Šæ•¸
        """
        self.previous_value = self.initial_cash
    

class agent_indicator(bt.Indicator):
    """
    SAC ä»£ç†æŒ‡æ¨™ - åƒ…æ¨ç†
    
    åŠŸèƒ½ï¼š
    1. åŠ è¼‰é è¨“ç·´çš„ SAC æ¨¡å‹
    2. æ§‹å»ºç‹€æ…‹å‘é‡ï¼ˆHullMA, RSI, CCI, åƒ¹æ ¼è®ŠåŒ–ç­‰ï¼‰
    3. ä½¿ç”¨ä»£ç†é¸æ“‡å‹•ä½œ
    4. è¼¸å‡ºäº¤æ˜“ä¿¡è™Ÿ
    """
    lines = ('agent_signal',)
    params = (
        ('model_config', SAC_CONFIG),
    )
    def __init__(self):
        # åˆå§‹åŒ–æŠ€è¡“æŒ‡æ¨™
        self.hullma = IndicatorFactory.hullma_indicator(self.data)
        self.rsi = IndicatorFactory.rsi(
            self.data, 
            period=self.params.model_config['rsi_period']
        )
        self.cci = IndicatorFactory.cci(
            self.data, 
            period=self.params.model_config['cci_period']
        )
        
        self.state_dim = self.params.model_config['state_dim']
        
        # å‰µå»º SAC ä»£ç†
        self.agent = create_sac_agent(
            state_dim=self.state_dim,
            action_dim=1,
            learning_rate=self.params.model_config['learning_rate'],
            gamma=self.params.model_config['gamma'],
            tau=self.params.model_config['tau'],
            alpha=self.params.model_config['alpha']
        )
        
        # ========== Z-score æ­£è¦åŒ–ç”¨çš„æ»¾å‹•çª—å£ ==========
        self.normalization_window = self.params.model_config.get('normalization_window', 100)
        self.state_history = deque(maxlen=self.normalization_window)
        
        # ========== åŠ è¼‰é è¨“ç·´æ¨¡å‹ ==========
        model_path = self.params.model_config['model_path']
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}ï¼Œè«‹å…ˆè¨“ç·´æ¨¡å‹")
        
        print(f"åŠ è¼‰ SAC æ¨¡å‹: {model_path}")
        self.agent.load(model_path)
        self.model_path = model_path
        print("æ¨¡å‹åŠ è¼‰å®Œæˆï¼")
        

    def _build_state(self):
        """æ§‹å»ºç‹€æ…‹å‘é‡ï¼ˆæœªæ­£è¦åŒ–ï¼‰"""
        current_price = self.data.close[0]
        
        # HMA çš„ä¸‰å€‹è¼¸å‡º
        mhull = self.hullma.mhull[0]
        shull = self.hullma.shull[0]
        signal = self.hullma.signal[0]
        
        # RSI å’Œ CCI
        rsi = self.rsi.rsi[0]
        cci = self.cci.cci[0]
        
        # å‰ n æ ¹ K ç·šçš„å„è‡ªè®ŠåŒ–ç‡
        period = self.p.model_config['price_change_period']
        price_changes = []
        
        for i in range(1, period + 1):
            if len(self) > i:
                price_change = (self.data.close[-i+1] - self.data.close[-i]) / self.data.close[-i]
            else:
                price_change = 0
            price_changes.append(price_change)
        
        # æˆäº¤é‡
        volume = self.data.volume[0]
        
        # çµ„åˆç‹€æ…‹å‘é‡ï¼šmhull + shull + signal + volume + current_price + price_changes + rsi + cci
        state_list = [
            mhull,
            shull,
            signal,
            volume,
            current_price
        ]
        state_list.extend(price_changes)
        state_list.extend([rsi, cci])
        
        return np.array(state_list, dtype=np.float32)
    
    def _normalize_state(self, state):
        """ä½¿ç”¨ Z-score æ­£è¦åŒ–ç‹€æ…‹å‘é‡"""
        # æ·»åŠ ç•¶å‰ç‹€æ…‹åˆ°æ­·å²è¨˜éŒ„
        self.state_history.append(state.copy())
        
        # å¦‚æœæ­·å²æ•¸æ“šä¸è¶³ï¼Œè¿”å›åŸå§‹ç‹€æ…‹ï¼ˆæˆ–å…¨é›¶ï¼‰
        if len(self.state_history) < 2:
            return np.zeros_like(state)
        
        # è¨ˆç®—æ­·å²æ•¸æ“šçš„ mean å’Œ std
        history_array = np.array(self.state_history)
        mean = np.mean(history_array, axis=0)
        std = np.std(history_array, axis=0)
        
        # é¿å…é™¤ä»¥é›¶ï¼Œå°æ–¼ std ç‚º 0 çš„ç‰¹å¾µï¼Œä¿æŒåŸå€¼
        std = np.where(std < 1e-8, 1.0, std)
        
        # Z-score æ­£è¦åŒ–
        normalized_state = (state - mean) / std
        
        return normalized_state

    def next(self):
        """æ¯æ ¹ K ç·šåŸ·è¡Œä¸€æ¬¡"""
        # æ§‹å»ºåŸå§‹ç‹€æ…‹
        raw_state = self._build_state()
        
        # Z-score æ­£è¦åŒ–
        normalized_state = self._normalize_state(raw_state)
        
        # ä½¿ç”¨æ­£è¦åŒ–å¾Œçš„ç‹€æ…‹é¸æ“‡å‹•ä½œ
        action = self.agent.select_action(normalized_state)
        
        # è¼¸å‡ºå‹•ä½œä¿¡è™Ÿ
        self.lines.agent_signal[0] = action[0]
        



def load_and_split_data(data_path, train_ratio=0.6, val_ratio=0.2):
    """
    è¼‰å…¥ä¸¦åˆ‡åˆ†è³‡æ–™
    
    Args:
        data_path: è³‡æ–™è·¯å¾‘
        train_ratio: è¨“ç·´é›†æ¯”ä¾‹
        val_ratio: é©—è­‰é›†æ¯”ä¾‹
        
    Returns:
        tuple: (train_df, val_df, test_df)
    """
    df = pd.read_csv(data_path)
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df.set_index('timestamp', inplace=True)
    
    n = len(df)
    train_end = int(n * train_ratio)
    val_end = int(n * (train_ratio + val_ratio))
    
    train_df = df.iloc[:train_end]
    val_df = df.iloc[train_end:val_end]
    test_df = df.iloc[val_end:]
    
    print(f"è³‡æ–™åˆ‡åˆ†:")
    print(f"  è¨“ç·´é›†: {len(train_df)} ç­† ({train_df.index[0]} ~ {train_df.index[-1]})")
    print(f"  é©—è­‰é›†: {len(val_df)} ç­† ({val_df.index[0]} ~ {val_df.index[-1]})")
    print(f"  æ¸¬è©¦é›†: {len(test_df)} ç­† ({test_df.index[0]} ~ {test_df.index[-1]})")
    
    return train_df, val_df, test_df


def run_training_episode(df, agent, trainer):
    """
    åŸ·è¡Œä¸€å€‹è¨“ç·´ episode
    
    Args:
        df: è¨“ç·´è³‡æ–™
        agent: SAC agent
        trainer: train_agent_indicator å¯¦ä¾‹
        
    Returns:
        dict: episode çµ±è¨ˆè³‡è¨Š
    """
    cerebro = bt.Cerebro()
    
    # æ·»åŠ è¨“ç·´ç­–ç•¥
    cerebro.addstrategy(TrainingStrategy, trainer=trainer, agent=agent)
    
    # æ·»åŠ è³‡æ–™
    data = bt.feeds.PandasData(
        dataname=df,
        datetime=None,
        open='open',
        high='high',
        low='low',
        close='close',
        volume='volume',
        openinterest=-1
    )
    cerebro.adddata(data)
    
    # è¨­å®š broker
    cerebro.broker.setcash(BROKER_CONFIG['initial_cash'])
    cerebro.broker.setcommission(commission=BROKER_CONFIG['commission'])
    
    # æ›´æ–° trainer çš„ cerebro å¼•ç”¨
    trainer.cerebro = cerebro
    trainer.reset_episode_tracking()
    
    # åŸ·è¡Œå›æ¸¬
    cerebro.run()
    
    # è¿”å›çµ±è¨ˆè³‡è¨Š
    final_value = cerebro.broker.getvalue()
    portfolio_return = (final_value - BROKER_CONFIG['initial_cash']) / BROKER_CONFIG['initial_cash']
    
    return {
        'final_value': final_value,
        'return': portfolio_return
    }


def validate(df, agent):
    """
    é©—è­‰æ¨¡å‹
    
    Args:
        df: é©—è­‰è³‡æ–™
        agent: SAC agent
        
    Returns:
        dict: é©—è­‰çµ±è¨ˆè³‡è¨Š
    """
    cerebro = bt.Cerebro()
    
    # å‰µå»ºè‡¨æ™‚ trainerï¼ˆä¸é€²è¡Œè¨“ç·´ï¼‰
    temp_trainer = train_agent_indicator(cerebro)
    
    # æ·»åŠ è¨“ç·´ç­–ç•¥ï¼ˆä½†ä¸è¨“ç·´ï¼‰
    cerebro.addstrategy(TrainingStrategy, trainer=None, agent=agent)
    
    # æ·»åŠ è³‡æ–™
    data = bt.feeds.PandasData(
        dataname=df,
        datetime=None,
        open='open',
        high='high',
        low='low',
        close='close',
        volume='volume',
        openinterest=-1
    )
    cerebro.adddata(data)
    
    # è¨­å®š broker
    cerebro.broker.setcash(BROKER_CONFIG['initial_cash'])
    cerebro.broker.setcommission(commission=BROKER_CONFIG['commission'])
    
    # åŸ·è¡Œå›æ¸¬
    cerebro.run()
    
    # è¿”å›çµ±è¨ˆè³‡è¨Š
    final_value = cerebro.broker.getvalue()
    portfolio_return = (final_value - BROKER_CONFIG['initial_cash']) / BROKER_CONFIG['initial_cash']
    
    return {
        'final_value': final_value,
        'return': portfolio_return
    }


def test_model(df, model_path):
    """
    ä½¿ç”¨æ¸¬è©¦é›†è©•ä¼°æ¨¡å‹
    
    Args:
        df: æ¸¬è©¦è³‡æ–™
        model_path: æ¨¡å‹è·¯å¾‘
        
    Returns:
        dict: æ¸¬è©¦çµ±è¨ˆè³‡è¨Š
    """
    print("\n" + "="*70)
    print("é–‹å§‹æ¸¬è©¦æ¨¡å‹")
    print("="*70)
    
    # å‰µå»º agentï¼ˆæ¨ç†æ¨¡å¼ï¼‰
    import torch
    device_config = TRAIN_CONFIG.get('device', 'auto')
    force_cuda = TRAIN_CONFIG.get('force_cuda', False)
    
    agent = create_sac_agent(
        state_dim=SAC_CONFIG['state_dim'],
        action_dim=SAC_CONFIG['action_dim'],
        learning_rate=SAC_CONFIG['learning_rate'],
        gamma=SAC_CONFIG['gamma'],
        tau=SAC_CONFIG['tau'],
        alpha=SAC_CONFIG['alpha'],
        train_mode=False,  # æ¨ç†æ¨¡å¼
        device=device_config,
        force_cuda=force_cuda
    )
    print(f"ä½¿ç”¨è¨­å‚™: {agent.device}")
    
    # è¼‰å…¥æœ€ä½³æ¨¡å‹
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return None
    
    print(f"è¼‰å…¥æ¨¡å‹: {model_path}")
    agent.load(model_path)
    
    # åŸ·è¡Œæ¸¬è©¦
    cerebro = bt.Cerebro()
    cerebro.addstrategy(TrainingStrategy, trainer=None, agent=agent)
    
    # æ·»åŠ è³‡æ–™
    data = bt.feeds.PandasData(
        dataname=df,
        datetime=None,
        open='open',
        high='high',
        low='low',
        close='close',
        volume='volume',
        openinterest=-1
    )
    cerebro.adddata(data)
    
    # è¨­å®š broker
    initial_cash = BROKER_CONFIG['initial_cash']
    cerebro.broker.setcash(initial_cash)
    cerebro.broker.setcommission(commission=BROKER_CONFIG['commission'])
    
    print(f"\næ¸¬è©¦è³‡æ–™: {len(df)} ç­† ({df.index[0]} ~ {df.index[-1]})")
    print(f"åˆå§‹è³‡é‡‘: ${initial_cash:,.2f}\n")
    
    # åŸ·è¡Œå›æ¸¬
    test_start_time = time.time()
    cerebro.run()
    test_time = time.time() - test_start_time
    
    # è¨ˆç®—çµ±è¨ˆ
    final_value = cerebro.broker.getvalue()
    portfolio_return = (final_value - initial_cash) / initial_cash
    profit = final_value - initial_cash
    
    # é¡¯ç¤ºæ¸¬è©¦çµæœ
    print("="*70)
    print("æ¸¬è©¦çµæœ")
    print("="*70)
    print(f"åˆå§‹è³‡é‡‘: ${initial_cash:,.2f}")
    print(f"æœ€çµ‚è³‡é‡‘: ${final_value:,.2f}")
    print(f"æ·¨åˆ©æ½¤:   ${profit:,.2f}")
    print(f"æ”¶ç›Šç‡:   {portfolio_return*100:.2f}%")
    print(f"æ¸¬è©¦æ™‚é–“: {test_time:.2f}s")
    print("="*70)
    
    # é¡¯ç¤ºå›æ¸¬åœ–è¡¨
    print("\næ­£åœ¨ç”Ÿæˆå›æ¸¬åœ–è¡¨...")
    try:
        cerebro.plot(style='candlestick')
    except Exception as e:
        print(f"åœ–è¡¨é¡¯ç¤ºå¤±æ•—: {e}")
        print("æç¤ºï¼šå¦‚æœåœ¨ç„¡GUIç’°å¢ƒä¸­é‹è¡Œï¼Œå¯èƒ½ç„¡æ³•é¡¯ç¤ºåœ–è¡¨")
    
    return {
        'initial_value': initial_cash,
        'final_value': final_value,
        'profit': profit,
        'return': portfolio_return,
        'test_time': test_time
    }


def train_sac_model(train_df, val_df, num_episodes=None, enable_validation=True):
    """
    è¨“ç·´ SAC æ¨¡å‹
    
    Args:
        train_df: è¨“ç·´è³‡æ–™
        val_df: é©—è­‰è³‡æ–™
        num_episodes: è¨“ç·´å›åˆæ•¸ï¼ˆè‹¥ç‚º Noneï¼Œå‰‡å¾ TRAIN_CONFIG è®€å–ï¼‰
        enable_validation: æ˜¯å¦å•Ÿç”¨é©—è­‰
    """
    if num_episodes is None:
        num_episodes = TRAIN_CONFIG['episodes']
    
    print("\n" + "="*70)
    print("é–‹å§‹è¨“ç·´ SAC æ¨¡å‹")
    print("="*70)
    
    # æª¢æŸ¥ä¸¦é¡¯ç¤ºè¨­å‚™ä¿¡æ¯
    import torch
    device_config = TRAIN_CONFIG.get('device', 'auto')
    force_cuda = TRAIN_CONFIG.get('force_cuda', False)
    
    print("\nğŸ–¥ï¸  è¨­å‚™é…ç½®:")
    print(f"  è¨­å‚™é¸é …: {device_config}")
    print(f"  å¼·åˆ¶ä½¿ç”¨ CUDA: {force_cuda}")
    print(f"  CUDA å¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"  GPU å‹è™Ÿ: {torch.cuda.get_device_name(0)}")
        print(f"  CUDA ç‰ˆæœ¬: {torch.version.cuda}")
    
    # å‰µå»º SAC agentï¼ˆè¨“ç·´æ¨¡å¼ï¼‰
    try:
        agent = create_sac_agent(
            state_dim=SAC_CONFIG['state_dim'],
            action_dim=SAC_CONFIG['action_dim'],
            learning_rate=SAC_CONFIG['learning_rate'],
            gamma=SAC_CONFIG['gamma'],
            tau=SAC_CONFIG['tau'],
            alpha=SAC_CONFIG['alpha'],
            train_mode=True,
            device=device_config,
            force_cuda=force_cuda
        )
        print(f"  âœ… ä½¿ç”¨è¨­å‚™: {agent.device}")
    except RuntimeError as e:
        print(f"\nâŒ è¨­å‚™é…ç½®éŒ¯èª¤: {e}")
        print("\næç¤º: å¦‚æœè¦ä½¿ç”¨ CPU è¨“ç·´ï¼Œè«‹åœ¨ config.py ä¸­è¨­ç½®:")
        print("  TRAIN_CONFIG['force_cuda'] = False")
        print("  æˆ–")
        print("  TRAIN_CONFIG['device'] = 'cpu'")
        raise
    
    # å‰µå»º trainer
    cerebro_placeholder = bt.Cerebro()  # ä½”ä½ç”¨
    trainer = train_agent_indicator(cerebro_placeholder)
    
    # è¨“ç·´å¾ªç’°
    best_val_return = -float('inf')
    no_improvement_count = 0
    start_time = time.time()
    
    for episode in range(1, num_episodes + 1):
        episode_start_time = time.time()
        
        # åŸ·è¡Œè¨“ç·´ episode
        stats = run_training_episode(train_df, agent, trainer)
        
        episode_time = time.time() - episode_start_time
        elapsed_time = time.time() - start_time
        
        # æ‰“å°è¨“ç·´é€²åº¦
        if episode % TRAIN_CONFIG['print_frequency'] == 0:
            avg_loss = np.mean([l['critic_loss'] for l in trainer.training_losses[-100:]]) if trainer.training_losses else 0
            print(f"\n[Episode {episode}/{num_episodes}]")
            print(f"  æ™‚é–“: {episode_time:.2f}s (ç¸½è¨ˆ: {elapsed_time/60:.2f}min)")
            print(f"  æ”¶ç›Šç‡: {stats['return']*100:.2f}%")
            print(f"  æœ€çµ‚è³‡é‡‘: ${stats['final_value']:.2f}")
            print(f"  Replay Buffer: {len(trainer.replay_buffer)}")
            print(f"  å¹³å‡ Critic Loss: {avg_loss:.4f}")
        
        # é©—è­‰
        if enable_validation and episode % TRAIN_CONFIG['print_frequency'] == 0:
            val_stats = validate(val_df, agent)
            val_return = val_stats['return']
            print(f"  é©—è­‰æ”¶ç›Šç‡: {val_return*100:.2f}%")
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºæœ€ä½³æ¨¡å‹
            if val_return > best_val_return:
                best_val_return = val_return
                no_improvement_count = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                os.makedirs(os.path.dirname(SAC_CONFIG['model_path']), exist_ok=True)
                agent.save(SAC_CONFIG['model_path'])
                print(f"  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (é©—è­‰æ”¶ç›Šç‡: {best_val_return*100:.2f}%)")
            else:
                no_improvement_count += 1
        
        # å®šæœŸä¿å­˜æ¨¡å‹
        if episode % TRAIN_CONFIG['save_frequency'] == 0:
            checkpoint_path = SAC_CONFIG['model_path'].replace('.pth', f'_ep{episode}.pth')
            os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)
            agent.save(checkpoint_path)
            print(f"  âœ“ ä¿å­˜æª¢æŸ¥é»: {checkpoint_path}")
        
        # æ—©åœæª¢æŸ¥
        if enable_validation and no_improvement_count >= TRAIN_CONFIG['early_stopping_patience']:
            print(f"\næ—©åœ: é©—è­‰æ”¶ç›Šç‡å·² {no_improvement_count} å€‹ episode æ²’æœ‰æ”¹å–„")
            break
        
        # æª¢æŸ¥æ˜¯å¦é”åˆ°ç›®æ¨™æ”¶ç›Šç‡
        if enable_validation and best_val_return >= TRAIN_CONFIG['target_return']:
            print(f"\né”åˆ°ç›®æ¨™æ”¶ç›Šç‡ {TRAIN_CONFIG['target_return']*100:.2f}%ï¼Œåœæ­¢è¨“ç·´")
            break
    
    total_time = time.time() - start_time
    print("\n" + "="*70)
    print(f"è¨“ç·´å®Œæˆï¼ç¸½æ™‚é–“: {total_time/60:.2f} åˆ†é˜")
    print(f"æœ€ä½³é©—è­‰æ”¶ç›Šç‡: {best_val_return*100:.2f}%")
    print(f"æ¨¡å‹å·²ä¿å­˜è‡³: {SAC_CONFIG['model_path']}")
    print("="*70)


if __name__ == '__main__':
    # è¼‰å…¥ä¸¦åˆ‡åˆ†è³‡æ–™
    data_path = DATA_CONFIG['data_path']
    train_df, val_df, test_df = load_and_split_data(
        data_path,
        train_ratio=DATA_CONFIG['train_ratio'],
        val_ratio=DATA_CONFIG['val_ratio']
    )
    
    # è¨“ç·´æ¨¡å‹
    train_sac_model(
        train_df, 
        val_df,
        num_episodes=TRAIN_CONFIG['episodes'],
        enable_validation=DATA_CONFIG['enable_validation']
    )
    
    # ä½¿ç”¨æ¸¬è©¦é›†è©•ä¼°æœ€ä½³æ¨¡å‹
    test_stats = test_model(test_df, SAC_CONFIG['model_path'])
    
    # æœ€çµ‚ç¸½çµ
    print("\n" + "="*70)
    print("è¨“ç·´èˆ‡æ¸¬è©¦æµç¨‹å®Œæˆï¼")
    print("="*70)
    if test_stats:
        print(f"âœ… æ¸¬è©¦é›†æ”¶ç›Šç‡: {test_stats['return']*100:.2f}%")
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜è‡³: {SAC_CONFIG['model_path']}")
    print("="*70)